{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gender and Age Classification From Audio\n",
        "## Cheolmin Oh(Jason Oh)\n",
        "### Project Description: I plan to determine gender and age by extracting features from audio recordings. I will utilize these features in a convolutional neural network (CNN) to develop a model capable of identifying gender and age. Finally, I will assess the model's performance using actual audio recordings to evaluate its effectiveness in real-world scenarios."
      ],
      "metadata": {
        "id": "8MzXJVB6cPRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quick overall summary:\n",
        "\n",
        "1. I have used Common Voice datasets. I am not using their api. I downloaded the datasets into my google drive.\n",
        "\n",
        "2. `common_test.tsv` contains gender and age labels and path to audio mp3. The path extracted from this tsv is used `/{your-folder}/en_test_0/{path}` to read over mp3 file in this folder.\n",
        "\n",
        "3. Initially, I experimented with MFCCs, but I found it challenging to understand how they work. Consequently, I switched to using spectrogram-based methods. I explored both spectrogram and Mel Spectrogram techniques. However, I encountered difficulties in managing a small dataset while retaining essential features when using the spectrogram method. As a result, I settled on utilizing Mel Spectrogram feature extraction for my CNN model."
      ],
      "metadata": {
        "id": "UqPYISQsdOrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mel-Spectrogram Implementation"
      ],
      "metadata": {
        "id": "-BNtDwIxBJn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I have initialized Google Drive for this project.\n",
        "\n",
        "The initial portion of these codes is focused on gender classification.\n",
        "I have implemented age classification in a separate section.\n",
        "\n",
        "I removed data labeled as 'None' and 'other,' resulting in a total of around 2300 data points.\n",
        "\n",
        "For age classification, I initially attempted using softmax for each label,\n",
        "but this approach resulted in very low accuracy for age prediction.\n",
        "Subsequently, I decided to transform it into a binary classification problem,\n",
        "predicting whether the voice belongs to the 0~30 age group or is older.\n",
        "However, the results were not very promising.\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Change audio-project to your folder name\n",
        "file_path = './drive/MyDrive/audio-project/common_test.tsv'\n",
        "\n",
        "# get full csv data\n",
        "df_raw = pd.read_csv(file_path, sep='\\t')\n",
        "print(len(df_raw))\n",
        "\n",
        "# gender parse\n",
        "# drop data with None values\n",
        "df_include_other = df_raw.dropna(subset=['gender'])\n",
        "print(len(df_include_other))\n",
        "# drop data with 'other' gender label\n",
        "df = df_include_other[df_include_other['gender'] != 'other']\n",
        "print(len(df))\n",
        "# drop age with None values\n",
        "df = df.dropna(subset=['age'])\n",
        "print(len(df))\n",
        "\n",
        "# age parse and display age types\n",
        "df_age_data = df_raw.dropna(subset=['age'])\n",
        "print(len(df_age_data))\n",
        "age_counts = df_age_data.groupby('age').size().reset_index(name='count')\n",
        "print(age_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUfZMyWolDwr",
        "outputId": "b0cf3095-1354-4f99-fc6f-71d6612c148b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "16372\n",
            "2356\n",
            "2320\n",
            "2308\n",
            "2373\n",
            "         age  count\n",
            "0   eighties      5\n",
            "1    fifties    109\n",
            "2   fourties    206\n",
            "3  seventies     37\n",
            "4    sixties     39\n",
            "5      teens    354\n",
            "6   thirties    465\n",
            "7   twenties   1158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "BDHD35_kKGdZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cbd1a13a-e493-47b9-90ca-3dfe95482250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            client_id  \\\n",
              "16  00c3f0e7c691ef30257d1bfa9adc410535b7ba3f48e344...   \n",
              "61  030d0b51d96c93d1db9e4ba94dceaf341d98d51eb36820...   \n",
              "79  040595ac714a98d21fe0c2f36d96997900085115175065...   \n",
              "84  043a451f648097c1a200f7e966289233e234f4e35ee00f...   \n",
              "86  0446e65032f30acdda12c87fef9d1de14d34946a4d2430...   \n",
              "\n",
              "                            path  \\\n",
              "16  common_voice_en_18295850.mp3   \n",
              "61  common_voice_en_22338655.mp3   \n",
              "79  common_voice_en_18277778.mp3   \n",
              "84  common_voice_en_21943181.mp3   \n",
              "86  common_voice_en_20586574.mp3   \n",
              "\n",
              "                                             sentence  up_votes  down_votes  \\\n",
              "16          The long-lived bridge still stands today.         2           0   \n",
              "61     The prints are then delivered to the customer.         3           1   \n",
              "79  We should not take for granted how fortunate w...         2           1   \n",
              "84                                              eight         4           3   \n",
              "86  Geils began playing jazz trumpet but eventuall...         4           0   \n",
              "\n",
              "         age  gender                      accents  variant locale    segment  \n",
              "16  twenties    male                          NaN      NaN     en        NaN  \n",
              "61  twenties  female            Hong Kong English      NaN     en        NaN  \n",
              "79  fourties    male        United States English      NaN     en        NaN  \n",
              "84  twenties    male                          NaN      NaN     en  Benchmark  \n",
              "86  twenties    male  United States English,wolof      NaN     en        NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88462a17-6694-4661-bb05-65ebf322f3a7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>path</th>\n",
              "      <th>sentence</th>\n",
              "      <th>up_votes</th>\n",
              "      <th>down_votes</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>accents</th>\n",
              "      <th>variant</th>\n",
              "      <th>locale</th>\n",
              "      <th>segment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>00c3f0e7c691ef30257d1bfa9adc410535b7ba3f48e344...</td>\n",
              "      <td>common_voice_en_18295850.mp3</td>\n",
              "      <td>The long-lived bridge still stands today.</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>twenties</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>030d0b51d96c93d1db9e4ba94dceaf341d98d51eb36820...</td>\n",
              "      <td>common_voice_en_22338655.mp3</td>\n",
              "      <td>The prints are then delivered to the customer.</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>twenties</td>\n",
              "      <td>female</td>\n",
              "      <td>Hong Kong English</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>040595ac714a98d21fe0c2f36d96997900085115175065...</td>\n",
              "      <td>common_voice_en_18277778.mp3</td>\n",
              "      <td>We should not take for granted how fortunate w...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>fourties</td>\n",
              "      <td>male</td>\n",
              "      <td>United States English</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>043a451f648097c1a200f7e966289233e234f4e35ee00f...</td>\n",
              "      <td>common_voice_en_21943181.mp3</td>\n",
              "      <td>eight</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>twenties</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>Benchmark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0446e65032f30acdda12c87fef9d1de14d34946a4d2430...</td>\n",
              "      <td>common_voice_en_20586574.mp3</td>\n",
              "      <td>Geils began playing jazz trumpet but eventuall...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>twenties</td>\n",
              "      <td>male</td>\n",
              "      <td>United States English,wolof</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88462a17-6694-4661-bb05-65ebf322f3a7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88462a17-6694-4661-bb05-65ebf322f3a7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88462a17-6694-4661-bb05-65ebf322f3a7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7a8941af-1d04-4a4d-8ff6-4a26369347ab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a8941af-1d04-4a4d-8ff6-4a26369347ab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7a8941af-1d04-4a4d-8ff6-4a26369347ab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These code are for feature extraction and model creation only for gender\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "from scipy.ndimage import zoom\n",
        "from tensorflow import keras\n",
        "\n",
        "def resize_spectrogram_interpolation(spectrogram, new_size):\n",
        "    return zoom(spectrogram, (new_size[0] / spectrogram.shape[0], new_size[1] / spectrogram.shape[1]))\n",
        "\n",
        "def create_spectrogram(input_mp3_file):\n",
        "    # I tried using 44.1k sample rate but didn't see difference.\n",
        "    # Probably because audio quality is not good enough.\n",
        "    # audio_signal, sample_rate = librosa.load(input_mp3_file, sr=44100)\n",
        "    audio_signal, sample_rate = librosa.load(input_mp3_file, sr=None) # 22.05k sample rate base\n",
        "    # https://librosa.org/doc/main/generated/librosa.effects.time_stretch.html\n",
        "    audio_signal = librosa.effects.time_stretch(y=audio_signal, rate=len(audio_signal)/sample_rate)\n",
        "\n",
        "    # https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html\n",
        "    melspectrogram = librosa.feature.melspectrogram(y=audio_signal, sr=sample_rate, n_fft=2048, hop_length=512)\n",
        "    spectrogram_db = librosa.power_to_db(S=melspectrogram, ref=1.0)\n",
        "\n",
        "    # # display the spectrogram\n",
        "    # plt.figure(figsize=(14, 5))\n",
        "    # librosa.display.specshow(spectrogram_db, sr=sample_rate, x_axis='time', y_axis='log')\n",
        "    # plt.colorbar()\n",
        "    # print(spectrogram_db.shape)\n",
        "\n",
        "    # # this method didn't have much difference in accuracy than using interpolation\n",
        "    # # add constant to smaller spectrograms\n",
        "    # target_size = 94\n",
        "    # if spectrogram_db.shape[1] < target_size:\n",
        "    #     pad_width = target_size - spectrogram_db.shape[1]\n",
        "    #     spectrogram_db = np.pad(spectrogram_db, ((0, 0), (0, pad_width)), mode='constant')\n",
        "    # spectrogram_db = spectrogram_db.reshape((128, 94, 1))\n",
        "\n",
        "    spectrogram_db = resize_spectrogram_interpolation(spectrogram_db, (128, 94))\n",
        "    return spectrogram_db\n",
        "\n",
        "features = [] # contains audio features in 2D array\n",
        "labels_gender = [] # contains label for gender('male' or 'female') same order as features\n",
        "\n",
        "# implemeted counter here more for testing things our in smaller portion\n",
        "counter = 0\n",
        "for _, row in df.iterrows():\n",
        "    if counter < 2320:  # max 2320\n",
        "        file_name = './drive/MyDrive/audio-project/en_test_0/' + row['path']\n",
        "\n",
        "        spectrogram = create_spectrogram(file_name)\n",
        "\n",
        "        # get the gender labels\n",
        "        if spectrogram is not None:\n",
        "            features.append(spectrogram)\n",
        "            if row['gender'].lower() == 'male':\n",
        "                labels_gender.append(0)\n",
        "            elif row['gender'].lower() == 'female':\n",
        "                labels_gender.append(1)\n",
        "            else:\n",
        "                print(\"Can't handle this gender label!\")\n",
        "        counter += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# make features, labels_gender to np array\n",
        "X = np.array(features)\n",
        "y = np.array(labels_gender)\n",
        "\n",
        "# split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# # test code to check what is passed in train or test\n",
        "# # mainly used to check how spectrogram looked like\n",
        "# # display X_train spectrogram\n",
        "# for i in range(len(X_train)):\n",
        "#     plt.figure(figsize=(8, 8))\n",
        "#     plt.imshow(X_train[i])\n",
        "#     plt.show()\n",
        "\n",
        "print(\"X_train dimensions:\", X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7im0fkck9Qi",
        "outputId": "03507fee-65b6-4b16-fb54-2ddb620ba3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train dimensions: (1731, 128, 94)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using model structure of https://keras.io/examples/vision/mnist_convnet/ for below models\n",
        "\n",
        "# I tried different approaches to find best model.\n",
        "# I believe having drop out is important to have, but too much drop out is not great.\n",
        "# I tried different things up to verion 4. From version 5, I tried to make it over fit.\n",
        "# Then made the model better.\n",
        "\n",
        "# # verision 1\n",
        "# # no dropout 0.5 > epoch 9 batch_size=32\n",
        "# # Test Loss: 0.4199875295162201, Test Accuracy: 0.8965517282485962\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(1, activation='sigmoid')) # using binary because determining 0 - male or 1 - female\n",
        "\n",
        "# version 2\n",
        "# yes dropout 0.5 > epoch 9 batch_size=32\n",
        "# Test Loss: 0.28963503241539, Test Accuracy: 0.9051724076271057\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid')) # using binary because determining 0 - male or 1 - female\n",
        "\n",
        "# # version 3\n",
        "# # epoch 14/20 batch_size=32\n",
        "# # Test Loss: 0.24547772109508514, Test Accuracy: 0.9172413945198059\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid')) # using binary because determining 0 - male or 1 - female\n",
        "\n",
        "# # version 4\n",
        "# epoch 15/20 batch_size=32\n",
        "# Test Loss: 0.26435738801956177, Test Accuracy: 0.8982758522033691\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid')) # using binary because determining 0 - male or 1 - female\n",
        "\n",
        "# version 5 - overfitting when no drop out, so adjusted with some dropout and dense until accuracies are about the same\n",
        "# also I initialially started with Conv2D(16,) it worked well but somewhat less accurate for longer audios\n",
        "# so I instead increased it to 64, 128, 256. 128 seems to give me most accurate result for random audio test\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(1024, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid')) # using binary because determining 0 - male or 1 - female\n",
        "\n",
        "# model.summary() # show model's summary\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# https://keras.io/api/callbacks/early_stopping/\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "# save the model\n",
        "model.save('gender_classification_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI6OwHIAojAa",
        "outputId": "0a405643-b995-441d-ad96-c8c2eebb7250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "22/22 [==============================] - 6s 197ms/step - loss: 5.9862 - accuracy: 0.7421 - val_loss: 0.4669 - val_accuracy: 0.8329\n",
            "Epoch 2/20\n",
            "22/22 [==============================] - 4s 183ms/step - loss: 0.4485 - accuracy: 0.8367 - val_loss: 0.5045 - val_accuracy: 0.8329\n",
            "Epoch 3/20\n",
            "22/22 [==============================] - 4s 182ms/step - loss: 0.3847 - accuracy: 0.8367 - val_loss: 0.3554 - val_accuracy: 0.8444\n",
            "Epoch 4/20\n",
            "22/22 [==============================] - 4s 183ms/step - loss: 0.3296 - accuracy: 0.8483 - val_loss: 0.3612 - val_accuracy: 0.8617\n",
            "Epoch 5/20\n",
            "22/22 [==============================] - 4s 187ms/step - loss: 0.3238 - accuracy: 0.8569 - val_loss: 0.3302 - val_accuracy: 0.8588\n",
            "Epoch 6/20\n",
            "22/22 [==============================] - 4s 184ms/step - loss: 0.2810 - accuracy: 0.8880 - val_loss: 0.3232 - val_accuracy: 0.8905\n",
            "Epoch 7/20\n",
            "22/22 [==============================] - 4s 186ms/step - loss: 0.2703 - accuracy: 0.8960 - val_loss: 0.2864 - val_accuracy: 0.9020\n",
            "Epoch 8/20\n",
            "22/22 [==============================] - 4s 185ms/step - loss: 0.2707 - accuracy: 0.8902 - val_loss: 0.2801 - val_accuracy: 0.8963\n",
            "Epoch 9/20\n",
            "22/22 [==============================] - 4s 185ms/step - loss: 0.2504 - accuracy: 0.8988 - val_loss: 0.2850 - val_accuracy: 0.8991\n",
            "Epoch 10/20\n",
            "22/22 [==============================] - 4s 185ms/step - loss: 0.2472 - accuracy: 0.8996 - val_loss: 0.2824 - val_accuracy: 0.8991\n",
            "Epoch 11/20\n",
            "22/22 [==============================] - 4s 185ms/step - loss: 0.2347 - accuracy: 0.9118 - val_loss: 0.2765 - val_accuracy: 0.8934\n",
            "Epoch 12/20\n",
            "22/22 [==============================] - 4s 183ms/step - loss: 0.2291 - accuracy: 0.9097 - val_loss: 0.2750 - val_accuracy: 0.9020\n",
            "Epoch 13/20\n",
            "22/22 [==============================] - 4s 183ms/step - loss: 0.2378 - accuracy: 0.9082 - val_loss: 0.3474 - val_accuracy: 0.8847\n",
            "Epoch 14/20\n",
            "22/22 [==============================] - 4s 184ms/step - loss: 0.2447 - accuracy: 0.8996 - val_loss: 0.2754 - val_accuracy: 0.8963\n",
            "Epoch 15/20\n",
            "22/22 [==============================] - 4s 177ms/step - loss: 0.2142 - accuracy: 0.9176 - val_loss: 0.2970 - val_accuracy: 0.8876\n",
            "19/19 [==============================] - 1s 25ms/step - loss: 0.2398 - accuracy: 0.9151\n",
            "Test Loss: 0.23981405794620514, Test Accuracy: 0.9150779843330383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "My project's primary objective was to develop a gender/age classification model\n",
        "and subsequently apply it to test various audio recordings for age determination.\n",
        "During the testing phase, I utilized some personal recordings and audios in wav format\n",
        "that I found online.\n",
        "'''\n",
        "\n",
        "# load model\n",
        "model = tf.keras.models.load_model('gender_classification_model.keras')\n",
        "\n",
        "def create_spectrogram_from_audio(audio, sr):\n",
        "    melspectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=2048, hop_length=512)\n",
        "    spectrogram_db = librosa.power_to_db(S=melspectrogram, ref=1.0)\n",
        "\n",
        "    spectrogram_db = resize_spectrogram_interpolation(spectrogram_db, (128, 94))\n",
        "    return spectrogram_db\n",
        "\n",
        "# segmenting the audios in 5 seconds and testing each of them\n",
        "# average prediction on these 5 seconds\n",
        "def predict_gender(audio_file, segment_duration=5, model=model):\n",
        "    audio, sr = librosa.load(audio_file, sr=None)\n",
        "    samples_per_segment = int(segment_duration * sr)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    if len(audio) < samples_per_segment:\n",
        "        segment = audio\n",
        "        spectrogram = create_spectrogram_from_audio(segment, sr)\n",
        "        features_reshaped = np.expand_dims(spectrogram, axis=0)\n",
        "        prediction = model.predict(features_reshaped)\n",
        "        predictions.append(prediction[0][0])\n",
        "    else:\n",
        "        # Calculate the number of full segments that can fit in the audio file\n",
        "        num_segments = len(audio) // samples_per_segment\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(num_segments):\n",
        "            start = i * samples_per_segment\n",
        "            end = start + samples_per_segment\n",
        "            segment = audio[start:end]\n",
        "\n",
        "            spectrogram = create_spectrogram_from_audio(segment, sr)\n",
        "            features_reshaped = np.expand_dims(spectrogram, axis=0)\n",
        "            prediction = model.predict(features_reshaped)\n",
        "            predictions.append(prediction[0][0])\n",
        "\n",
        "    avg_prediction = np.mean(predictions)\n",
        "    print(predictions)\n",
        "\n",
        "    if avg_prediction >= 0.5:\n",
        "        gender = \"Female\"\n",
        "    else:\n",
        "        gender = \"Male\"\n",
        "\n",
        "    return gender, avg_prediction\n",
        "\n",
        "# sample audio: You can use any format of audio of your choice to test this.\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/en_test_0/common_voice_en_18277778.mp3') # male\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/en_test_0/common_voice_en_22338655.mp3') # female\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/Recording.m4a') # me\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/sally.m4a') # female\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/Speaker_0000_00000.wav') # male\n",
        "gender_prediction = predict_gender('./drive/MyDrive/audio-project/Speaker_0000_00001.wav') # male\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/Speaker_0000_00002.wav') # male\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/Speaker_0001_00000.wav') # male\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/Speaker0048_000.wav') # female\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/Speaker0048_028.wav') # female\n",
        "\n",
        "print(f\"Predicted Gender: {gender_prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q085SStlBJ7",
        "outputId": "62039e1e-1a5d-4b31-9168-e25893da87ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 238ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "[0.43388322, 0.6385415, 0.21062468, 0.7336307, 0.2577917, 0.3255142, 0.2294937, 0.3939412, 0.58805007, 0.30647746, 0.09952997, 0.1183857]\n",
            "Predicted Gender: ('Male', 0.36132202)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Start of age implementation (Mel-Spectrogram)**\n",
        "\n",
        "I have used above code and version 5 CNN model."
      ],
      "metadata": {
        "id": "M6LqbGDCkMcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This part is very similar to what I have done above.\n",
        "\n",
        "The result was not very good. I believe this is due to lack of datasets.\n",
        "\n",
        "I will need to figure out a way to utilize more than 2300 datasets. Going something\n",
        "larger probably can't be done in Google Colab as it already uses up most resources.\n",
        "'''\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D, Input\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "from scipy.ndimage import zoom\n",
        "from tensorflow import keras\n",
        "\n",
        "def resize_spectrogram_interpolation(spectrogram, new_size):\n",
        "    return zoom(spectrogram, (new_size[0] / spectrogram.shape[0], new_size[1] / spectrogram.shape[1]))\n",
        "\n",
        "def create_spectrogram(input_mp3_file):\n",
        "    audio_signal, sample_rate = librosa.load(input_mp3_file, sr=None)\n",
        "    audio_signal = librosa.effects.time_stretch(y=audio_signal, rate=len(audio_signal)/sample_rate)\n",
        "\n",
        "    melspectrogram = librosa.feature.melspectrogram(y=audio_signal, sr=sample_rate, n_fft=2048, hop_length=512)\n",
        "    spectrogram_db = librosa.power_to_db(S=melspectrogram, ref=1.0)\n",
        "\n",
        "    # # display the spectrogram\n",
        "    # plt.figure(figsize=(14, 5))\n",
        "    # librosa.display.specshow(spectrogram_db, sr=sample_rate, x_axis='time', y_axis='log')\n",
        "    # plt.colorbar()\n",
        "    # print(spectrogram_db.shape)\n",
        "\n",
        "    # # add constant 0 to smaller spectrograms\n",
        "    # target_size = 94\n",
        "    # if spectrogram_db.shape[1] < target_size:\n",
        "    #     pad_width = target_size - spectrogram_db.shape[1]\n",
        "    #     spectrogram_db = np.pad(spectrogram_db, ((0, 0), (0, pad_width)), mode='constant')\n",
        "    # spectrogram_db = spectrogram_db.reshape((128, 94, 1))\n",
        "\n",
        "    spectrogram_db = resize_spectrogram_interpolation(spectrogram_db, (128, 94))\n",
        "    return spectrogram_db\n",
        "\n",
        "# tried to split 0~30 years old and 30+\n",
        "# mapping so I can binary classificaiton\n",
        "age_mapping = {\n",
        "    'teens': 0,\n",
        "    'twenties': 0,\n",
        "    'thirties': 1,\n",
        "    'fourties': 1,\n",
        "    'fifties': 1,\n",
        "    'sixties': 1,\n",
        "    'seventies': 1,\n",
        "    'eighties': 1\n",
        "}\n",
        "\n",
        "features = [] # contains audio features in 2D array\n",
        "labels_gender = [] # contains label for gender('male' or 'female') same order as features\n",
        "labels_age = [] # contains label for age(0 or 1) same order as features\n",
        "\n",
        "counter = 0\n",
        "for _, row in df.iterrows():\n",
        "    if counter < 2308:  # max 2308 for age parsed\n",
        "        file_name = './drive/MyDrive/audio-project/en_test_0/' + row['path']\n",
        "\n",
        "        spectrogram = create_spectrogram(file_name)\n",
        "\n",
        "        # get the gender labels\n",
        "        if spectrogram is not None:\n",
        "            features.append(spectrogram)\n",
        "            if row['gender'].lower() == 'male':\n",
        "                labels_gender.append(0)\n",
        "            elif row['gender'].lower() == 'female':\n",
        "                labels_gender.append(1)\n",
        "            else:\n",
        "                print(\"Can't handle this gender label!\")\n",
        "            labels_age.append(age_mapping[row['age']])\n",
        "        counter += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# make features, labels_gender to np array\n",
        "X = np.array(features)\n",
        "y_gender = np.array(labels_gender)\n",
        "y_age = np.array(labels_age)\n",
        "\n",
        "# split train and test\n",
        "X_train, X_test, y_train_gender, y_test_gender, y_train_age, y_test_age = train_test_split(X, y_gender, y_age, test_size=0.2)\n",
        "\n",
        "# # display X_train spectrogram\n",
        "# for i in range(len(X_train)):\n",
        "#     plt.figure(figsize=(8, 8))\n",
        "#     plt.imshow(X_train[i])\n",
        "#     plt.show()\n",
        "\n",
        "print(\"X_train dimensions:\", X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_sjPzfJSBfH",
        "outputId": "cb335d1a-64ce-4ee4-c76b-4dfd2afc74ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train dimensions: (1846, 128, 94)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy of version 5 model created previously\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(1024, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Got some help from https://stackoverflow.com/questions/44036971/multiple-outputs-in-keras\n",
        "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2], 1))\n",
        "x = model(input_layer)\n",
        "gender = Dense(1, activation='sigmoid', name='gender')(x)\n",
        "age = Dense(1, activation='sigmoid', name='age')(x)\n",
        "model = Model(inputs=input_layer, outputs=[gender, age])\n",
        "\n",
        "# model.summary() # show model's summary\n",
        "\n",
        "model.compile(loss={'gender': 'binary_crossentropy', 'age': 'binary_crossentropy'},\n",
        "              optimizer='adam',\n",
        "              metrics={'gender': 'accuracy', 'age': 'accuracy'})\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model.fit(X_train, {'gender': y_train_gender, 'age': y_train_age},\n",
        "                    validation_data=(X_test, {'gender': y_test_gender, 'age': y_test_age}),\n",
        "                    epochs=20, batch_size=64, callbacks=[early_stopping])\n",
        "\n",
        "eval_results = model.evaluate(X_test, {'gender': y_test_gender, 'age': y_test_age})\n",
        "print(f'Age Test Loss: {eval_results[0]}, Gender Test Loss: {eval_results[1]}, Gender Accuracy: {eval_results[3]}, Age Accuracy: {eval_results[4]}')\n",
        "\n",
        "# save the model\n",
        "model.save('gender_age_classification_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWEytV1AUbIJ",
        "outputId": "2c02f35d-725f-4f89-cda9-3e4541756a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "29/29 [==============================] - 7s 194ms/step - loss: 11.7749 - gender_loss: 4.1684 - age_loss: 7.6064 - gender_accuracy: 0.8126 - age_accuracy: 0.5796 - val_loss: 1.2104 - val_gender_loss: 0.5483 - val_age_loss: 0.6621 - val_gender_accuracy: 0.7879 - val_age_accuracy: 0.6385\n",
            "Epoch 2/20\n",
            "29/29 [==============================] - 5s 180ms/step - loss: 1.1177 - gender_loss: 0.4586 - age_loss: 0.6591 - gender_accuracy: 0.8294 - age_accuracy: 0.6387 - val_loss: 1.2005 - val_gender_loss: 0.5351 - val_age_loss: 0.6654 - val_gender_accuracy: 0.7879 - val_age_accuracy: 0.6385\n",
            "Epoch 3/20\n",
            "29/29 [==============================] - 5s 179ms/step - loss: 1.0920 - gender_loss: 0.4351 - age_loss: 0.6569 - gender_accuracy: 0.8294 - age_accuracy: 0.6387 - val_loss: 1.1109 - val_gender_loss: 0.4538 - val_age_loss: 0.6571 - val_gender_accuracy: 0.7879 - val_age_accuracy: 0.6385\n",
            "Epoch 4/20\n",
            "29/29 [==============================] - 5s 181ms/step - loss: 1.0107 - gender_loss: 0.3506 - age_loss: 0.6601 - gender_accuracy: 0.8315 - age_accuracy: 0.6376 - val_loss: 1.0139 - val_gender_loss: 0.3575 - val_age_loss: 0.6564 - val_gender_accuracy: 0.8268 - val_age_accuracy: 0.6385\n",
            "Epoch 5/20\n",
            "29/29 [==============================] - 5s 184ms/step - loss: 0.9614 - gender_loss: 0.3041 - age_loss: 0.6573 - gender_accuracy: 0.8667 - age_accuracy: 0.6311 - val_loss: 1.0256 - val_gender_loss: 0.3196 - val_age_loss: 0.7059 - val_gender_accuracy: 0.8615 - val_age_accuracy: 0.5108\n",
            "Epoch 6/20\n",
            "29/29 [==============================] - 5s 183ms/step - loss: 0.9365 - gender_loss: 0.2824 - age_loss: 0.6541 - gender_accuracy: 0.8922 - age_accuracy: 0.6300 - val_loss: 1.0703 - val_gender_loss: 0.3597 - val_age_loss: 0.7107 - val_gender_accuracy: 0.8571 - val_age_accuracy: 0.5238\n",
            "Epoch 7/20\n",
            "29/29 [==============================] - 5s 184ms/step - loss: 0.9459 - gender_loss: 0.2945 - age_loss: 0.6515 - gender_accuracy: 0.8857 - age_accuracy: 0.6392 - val_loss: 0.9928 - val_gender_loss: 0.3274 - val_age_loss: 0.6654 - val_gender_accuracy: 0.8723 - val_age_accuracy: 0.6126\n",
            "Epoch 8/20\n",
            "29/29 [==============================] - 5s 186ms/step - loss: 0.8979 - gender_loss: 0.2603 - age_loss: 0.6375 - gender_accuracy: 0.9041 - age_accuracy: 0.6295 - val_loss: 0.9339 - val_gender_loss: 0.2998 - val_age_loss: 0.6341 - val_gender_accuracy: 0.8831 - val_age_accuracy: 0.6385\n",
            "Epoch 9/20\n",
            "29/29 [==============================] - 5s 181ms/step - loss: 0.8826 - gender_loss: 0.2499 - age_loss: 0.6327 - gender_accuracy: 0.9003 - age_accuracy: 0.6387 - val_loss: 0.9157 - val_gender_loss: 0.2925 - val_age_loss: 0.6232 - val_gender_accuracy: 0.8939 - val_age_accuracy: 0.6385\n",
            "Epoch 10/20\n",
            "29/29 [==============================] - 5s 181ms/step - loss: 0.8783 - gender_loss: 0.2459 - age_loss: 0.6324 - gender_accuracy: 0.9030 - age_accuracy: 0.6408 - val_loss: 0.9521 - val_gender_loss: 0.3206 - val_age_loss: 0.6314 - val_gender_accuracy: 0.8615 - val_age_accuracy: 0.6385\n",
            "Epoch 11/20\n",
            "29/29 [==============================] - 5s 182ms/step - loss: 0.8896 - gender_loss: 0.2517 - age_loss: 0.6379 - gender_accuracy: 0.9068 - age_accuracy: 0.6278 - val_loss: 0.9433 - val_gender_loss: 0.3103 - val_age_loss: 0.6331 - val_gender_accuracy: 0.8918 - val_age_accuracy: 0.6385\n",
            "Epoch 12/20\n",
            "29/29 [==============================] - 5s 180ms/step - loss: 0.8691 - gender_loss: 0.2412 - age_loss: 0.6279 - gender_accuracy: 0.9095 - age_accuracy: 0.6398 - val_loss: 0.9241 - val_gender_loss: 0.2998 - val_age_loss: 0.6243 - val_gender_accuracy: 0.8853 - val_age_accuracy: 0.6667\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.9157 - gender_loss: 0.2925 - age_loss: 0.6232 - gender_accuracy: 0.8939 - age_accuracy: 0.6385\n",
            "Age Test Loss: 0.9156877398490906, Gender Test Loss: 0.2924714982509613, Gender Accuracy: 0.8939393758773804, Age Accuracy: 0.6385281682014465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "from scipy.ndimage import zoom\n",
        "from tensorflow import keras\n",
        "model = tf.keras.models.load_model('gender_age_classification_model.keras')\n",
        "\n",
        "# I asked gpt to fit the codes to age version using my original code\n",
        "def create_spectrogram_from_audio(audio, sr):\n",
        "    melspectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=2048, hop_length=512)\n",
        "    spectrogram_db = librosa.power_to_db(S=melspectrogram, ref=1.0)\n",
        "\n",
        "    spectrogram_db = resize_spectrogram_interpolation(spectrogram_db, (128, 94))\n",
        "    return spectrogram_db\n",
        "\n",
        "def predict_gender_age(audio_file, segment_duration=5, model=model):\n",
        "    audio, sr = librosa.load(audio_file, sr=None)\n",
        "    samples_per_segment = int(segment_duration * sr)\n",
        "\n",
        "    gender_predictions = []\n",
        "    age_predictions = []\n",
        "\n",
        "    if len(audio) < samples_per_segment:\n",
        "        segment = audio\n",
        "        spectrogram = create_spectrogram_from_audio(segment, sr)\n",
        "        features_reshaped = np.expand_dims(spectrogram, axis=0)\n",
        "        prediction = model.predict(features_reshaped)\n",
        "        gender_predictions.append(prediction[0][0])\n",
        "        age_predictions.append(prediction[1][0])\n",
        "    else:\n",
        "        num_segments = len(audio) // samples_per_segment\n",
        "\n",
        "        for i in range(num_segments):\n",
        "            start = i * samples_per_segment\n",
        "            end = start + samples_per_segment\n",
        "            segment = audio[start:end]\n",
        "\n",
        "            spectrogram = create_spectrogram_from_audio(segment, sr)\n",
        "            features_reshaped = np.expand_dims(spectrogram, axis=0)\n",
        "            prediction = model.predict(features_reshaped)\n",
        "            gender_predictions.append(prediction[0][0])\n",
        "            age_predictions.append(prediction[1][0])\n",
        "\n",
        "    avg_gender_prediction = np.mean(gender_predictions)\n",
        "    avg_age_prediction = np.mean(age_predictions)\n",
        "\n",
        "    gender = \"Female\" if avg_gender_prediction >= 0.5 else \"Male\"\n",
        "    age_group = \"Below 30\" if avg_age_prediction < 0.5 else \"Above 30\"\n",
        "\n",
        "    return gender, avg_gender_prediction, age_group, avg_age_prediction\n",
        "\n",
        "# gender, gender_confidence, age_group, age_confidence = predict_gender_age('./drive/MyDrive/audio-project/en_test_0/common_voice_en_18277778.mp3') # male below 30\n",
        "# gender, gender_confidence, age_group, age_confidence = predict_gender_age('./drive/MyDrive/audio-project/en_test_0/common_voice_en_22338655.mp3') # female below 30\n",
        "gender, gender_confidence, age_group, age_confidence = predict_gender_age('./Recording.m4a') # male below 30\n",
        "# gender, gender_confidence, age_group, age_confidence = predict_gender_age('./drive/MyDrive/audio-project/sally.m4a') # female above 30\n",
        "# gender, gender_confidence, age_group, age_confidence = predict_gender_age('./drive/MyDrive/audio-project/Speaker_0000_00000.wav')\n",
        "# gender, gender_confidence, age_group, age_confidence = predict_gender_age('./drive/MyDrive/audio-project/Speaker_0000_00001.wav')\n",
        "# gender, gender_confidence, age_group, age_confidence = predict_gender_age('./drive/MyDrive/audio-project/Speaker_0000_00002.wav')\n",
        "# gender, gender_confidence, age_group, age_confidence = predict_gender_age('./drive/MyDrive/audio-project/Speaker_0001_00000.wav')\n",
        "# gender, gender_confidence, age_group, age_confidence = predict_gender_age('./drive/MyDrive/audio-project/Speaker0048_000.wav')\n",
        "# gender, gender_confidence, age_group, age_confidence = predict_gender_age('./drive/MyDrive/audio-project/Speaker0048_028.wav')\n",
        "print(f\"Predicted Gender: {gender}, Result: {gender_confidence}\")\n",
        "print(f\"Predicted Age Group: {age_group}, Result: {age_confidence}\")"
      ],
      "metadata": {
        "id": "Rdygmwd5X52y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Below here are MFCCS and Spectrogram Implementations**\n",
        "\n",
        "## These are here just for future reference. The code is not completed or working well.\n",
        "\n",
        "MFCCS I will need more research on it before utilizing it.\n",
        "\n",
        "Spectrogram implementation is something I tried before Mel-Spectrogram, which didn't work well."
      ],
      "metadata": {
        "id": "tDmjt91Hbklt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MFCCS implementation\n",
        "\n",
        "I think this is better than mel-spectrogram when used correctly. However, I am not sure what the MFCCS feature is calculated. So, I have not used it."
      ],
      "metadata": {
        "id": "bh8m9wzrAUeb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPGnfGgkFWui",
        "outputId": "0c8d48f0-3dad-467f-c39d-d55c1b8d7207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMHCuQmtykfV",
        "outputId": "b806190d-fd4e-4636-bc52-309df497d14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16372\n",
            "2356\n",
            "2320\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "file_path = './drive/MyDrive/audio-project/common_test.tsv'\n",
        "# file_path = './drive/MyDrive/audio-project/common_train.tsv'\n",
        "df_raw = pd.read_csv(file_path, sep='\\t')\n",
        "print(len(df_raw))\n",
        "df_include_other = df_raw.dropna(subset=['gender'])\n",
        "print(len(df_include_other))\n",
        "df = df_include_other[df_include_other['gender'] != 'other']\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "Mht6msPP0hqq",
        "outputId": "9eb4faee-bf38-4ec9-fd00-a719c8d710ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            client_id  \\\n",
              "16  00c3f0e7c691ef30257d1bfa9adc410535b7ba3f48e344...   \n",
              "61  030d0b51d96c93d1db9e4ba94dceaf341d98d51eb36820...   \n",
              "79  040595ac714a98d21fe0c2f36d96997900085115175065...   \n",
              "84  043a451f648097c1a200f7e966289233e234f4e35ee00f...   \n",
              "86  0446e65032f30acdda12c87fef9d1de14d34946a4d2430...   \n",
              "\n",
              "                            path  \\\n",
              "16  common_voice_en_18295850.mp3   \n",
              "61  common_voice_en_22338655.mp3   \n",
              "79  common_voice_en_18277778.mp3   \n",
              "84  common_voice_en_21943181.mp3   \n",
              "86  common_voice_en_20586574.mp3   \n",
              "\n",
              "                                             sentence  up_votes  down_votes  \\\n",
              "16          The long-lived bridge still stands today.         2           0   \n",
              "61     The prints are then delivered to the customer.         3           1   \n",
              "79  We should not take for granted how fortunate w...         2           1   \n",
              "84                                              eight         4           3   \n",
              "86  Geils began playing jazz trumpet but eventuall...         4           0   \n",
              "\n",
              "         age  gender                      accents  variant locale    segment  \n",
              "16  twenties    male                          NaN      NaN     en        NaN  \n",
              "61  twenties  female            Hong Kong English      NaN     en        NaN  \n",
              "79  fourties    male        United States English      NaN     en        NaN  \n",
              "84  twenties    male                          NaN      NaN     en  Benchmark  \n",
              "86  twenties    male  United States English,wolof      NaN     en        NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-585b323e-d861-4e7c-91bd-53ac2ac67569\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>path</th>\n",
              "      <th>sentence</th>\n",
              "      <th>up_votes</th>\n",
              "      <th>down_votes</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>accents</th>\n",
              "      <th>variant</th>\n",
              "      <th>locale</th>\n",
              "      <th>segment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>00c3f0e7c691ef30257d1bfa9adc410535b7ba3f48e344...</td>\n",
              "      <td>common_voice_en_18295850.mp3</td>\n",
              "      <td>The long-lived bridge still stands today.</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>twenties</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>030d0b51d96c93d1db9e4ba94dceaf341d98d51eb36820...</td>\n",
              "      <td>common_voice_en_22338655.mp3</td>\n",
              "      <td>The prints are then delivered to the customer.</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>twenties</td>\n",
              "      <td>female</td>\n",
              "      <td>Hong Kong English</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>040595ac714a98d21fe0c2f36d96997900085115175065...</td>\n",
              "      <td>common_voice_en_18277778.mp3</td>\n",
              "      <td>We should not take for granted how fortunate w...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>fourties</td>\n",
              "      <td>male</td>\n",
              "      <td>United States English</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>043a451f648097c1a200f7e966289233e234f4e35ee00f...</td>\n",
              "      <td>common_voice_en_21943181.mp3</td>\n",
              "      <td>eight</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>twenties</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>Benchmark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0446e65032f30acdda12c87fef9d1de14d34946a4d2430...</td>\n",
              "      <td>common_voice_en_20586574.mp3</td>\n",
              "      <td>Geils began playing jazz trumpet but eventuall...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>twenties</td>\n",
              "      <td>male</td>\n",
              "      <td>United States English,wolof</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-585b323e-d861-4e7c-91bd-53ac2ac67569')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-585b323e-d861-4e7c-91bd-53ac2ac67569 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-585b323e-d861-4e7c-91bd-53ac2ac67569');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-119e04ee-7225-4ae9-a562-847838edd3d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-119e04ee-7225-4ae9-a562-847838edd3d1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-119e04ee-7225-4ae9-a562-847838edd3d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEU44p-35drV"
      },
      "outputs": [],
      "source": [
        "# !tar -xvf './drive/MyDrive/audio-project/en_test_0.tar' -C './drive/MyDrive/audio-project/' # unzip already done!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3NRWRX584YH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c7e504-09b1-4cd0-c272-66260f548456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train dimensions: (1600, 40, 1)\n",
            "Epoch 1/20\n",
            "40/40 [==============================] - 1s 12ms/step - loss: 0.6920 - accuracy: 0.7859 - val_loss: 0.3958 - val_accuracy: 0.8313\n",
            "Epoch 2/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.3905 - accuracy: 0.8453 - val_loss: 0.3413 - val_accuracy: 0.8406\n",
            "Epoch 3/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.3549 - accuracy: 0.8703 - val_loss: 0.3336 - val_accuracy: 0.8438\n",
            "Epoch 4/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.3448 - accuracy: 0.8703 - val_loss: 0.3190 - val_accuracy: 0.8562\n",
            "Epoch 5/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.3243 - accuracy: 0.8805 - val_loss: 0.3110 - val_accuracy: 0.8656\n",
            "Epoch 6/20\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3073 - accuracy: 0.8758 - val_loss: 0.3070 - val_accuracy: 0.8719\n",
            "Epoch 7/20\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3144 - accuracy: 0.8813 - val_loss: 0.3049 - val_accuracy: 0.8656\n",
            "Epoch 8/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.3096 - accuracy: 0.8883 - val_loss: 0.3233 - val_accuracy: 0.8625\n",
            "Epoch 9/20\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3106 - accuracy: 0.8930 - val_loss: 0.3229 - val_accuracy: 0.8656\n",
            "Epoch 10/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.2997 - accuracy: 0.8977 - val_loss: 0.3061 - val_accuracy: 0.8656\n",
            "Epoch 11/20\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.2780 - accuracy: 0.8969 - val_loss: 0.2940 - val_accuracy: 0.8594\n",
            "Epoch 12/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.2832 - accuracy: 0.8945 - val_loss: 0.2970 - val_accuracy: 0.8500\n",
            "Epoch 13/20\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.2796 - accuracy: 0.9047 - val_loss: 0.2974 - val_accuracy: 0.8781\n",
            "Epoch 14/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.2780 - accuracy: 0.9016 - val_loss: 0.2879 - val_accuracy: 0.8750\n",
            "Epoch 15/20\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.2581 - accuracy: 0.8984 - val_loss: 0.2885 - val_accuracy: 0.8687\n",
            "Epoch 16/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.2420 - accuracy: 0.9008 - val_loss: 0.3102 - val_accuracy: 0.8656\n",
            "Epoch 17/20\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.2661 - accuracy: 0.9008 - val_loss: 0.2907 - val_accuracy: 0.8687\n",
            "Epoch 18/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.2462 - accuracy: 0.9094 - val_loss: 0.2891 - val_accuracy: 0.8750\n",
            "Epoch 19/20\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.2355 - accuracy: 0.9180 - val_loss: 0.2977 - val_accuracy: 0.8719\n",
            "Epoch 20/20\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.2170 - accuracy: 0.9125 - val_loss: 0.2970 - val_accuracy: 0.8719\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2452 - accuracy: 0.9000\n",
            "Test Loss: 0.24523252248764038, Test Accuracy: 0.8999999761581421\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "# feature extraction function\n",
        "def extract_features(file_path):\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_path)\n",
        "        # https://librosa.org/doc/main/generated/librosa.feature.mfcc.html\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40) # can increase n_mfcc for longer array\n",
        "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
        "        return mfccs_processed\n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file_path)\n",
        "        return None\n",
        "\n",
        "features = [] # contains audio features in 1D array\n",
        "labels_gender = [] # this will contain label for gender 1 to 1 of features\n",
        "\n",
        "# for _, row in df.iterrows():\n",
        "#     file_name = './drive/MyDrive/audio-project/en_test_0/' + row['path']\n",
        "#     mfccs = extract_features(file_name)\n",
        "#     if mfccs is not None:\n",
        "#         features.append(mfccs)\n",
        "#         if row['gender'].lower() == 'male':\n",
        "#             labels_gender.append(0)\n",
        "#         elif row['gender'].lower() == 'female':\n",
        "#             labels_gender.append(1)\n",
        "#         else:\n",
        "#             print(f\"Unexpected gender label: {row['gender']}\")\n",
        "counter = 0\n",
        "for _, row in df.iterrows():\n",
        "    if counter < 2000:  # only 5 audio\n",
        "        file_name = './drive/MyDrive/audio-project/en_test_0/' + row['path']\n",
        "        mfccs = extract_features(file_name)\n",
        "        if mfccs is not None:\n",
        "            features.append(mfccs)\n",
        "            if row['gender'].lower() == 'male':\n",
        "                labels_gender.append(0)\n",
        "            elif row['gender'].lower() == 'female':\n",
        "                labels_gender.append(1)\n",
        "            else:\n",
        "                print(f\"Unexpected gender label: {row['gender']}\")\n",
        "        counter += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# make features, labels_gender to np array\n",
        "X = np.array(features) # 2D array\n",
        "y = np.array(labels_gender)\n",
        "\n",
        "# split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# reshape this to 3D to use Conv1D\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "print(\"X_train dimensions:\", X_train.shape)\n",
        "\n",
        "# CNN Model using Conv1D and MaxPooling1D\n",
        "# I chose this method because a lot of people in stackoverflow seemed to use Conv1D and MaxPooling for speach recognition\n",
        "# https://keras.io/api/layers/convolution_layers/convolution1d/\n",
        "# https://keras.io/keras_core/api/layers/pooling_layers/max_pooling1d/\n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(64, 3, activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid')) # using binary because determining 0 - male or 1 - female\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "# save the model\n",
        "model.save('gender_classification_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSyMHQhTaz86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d161ac50-5f35-44e6-f208-773035e5d6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x782b186ec820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 141ms/step\n",
            "[[0.74120694]]\n",
            "Predicted Gender: Female\n"
          ]
        }
      ],
      "source": [
        "# load model\n",
        "model = tf.keras.models.load_model('gender_classification_model.keras')\n",
        "\n",
        "# def extract_features(file_path, sr=44100):\n",
        "#     max_length = 120\n",
        "#     try:\n",
        "#         audio, sample_rate = librosa.load(file_path, sr=sr)\n",
        "#         stft = np.abs(librosa.stft(audio, n_fft=255, hop_length=2048))\n",
        "#         print(stft.shape)\n",
        "#         if stft.shape[1] < max_length:\n",
        "#             pad_width = max_length - stft.shape[1]\n",
        "#             stft = np.pad(stft, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "#         else:\n",
        "#             stft = stft[:, 720:max_length+720]\n",
        "\n",
        "#         return stft\n",
        "#     except Exception as e:\n",
        "#         print(\"Error encountered while parsing file: \", file_path)\n",
        "#         return None\n",
        "\n",
        "def predict_gender(audio_file):\n",
        "    features = extract_features(audio_file)\n",
        "    # features = resize_spectrogram(features, target_shape=(128, 128))\n",
        "\n",
        "    # reshape feature for the model\n",
        "    features_reshaped = np.expand_dims(features, axis=0)\n",
        "    features_reshaped = np.expand_dims(features_reshaped, axis=-1)\n",
        "\n",
        "    # get the prediction\n",
        "    prediction = model.predict(features_reshaped)\n",
        "    print(prediction)\n",
        "\n",
        "    if prediction[0][0] >= 0.5:\n",
        "        return \"Female\"\n",
        "    else:\n",
        "        return \"Male\"\n",
        "\n",
        "# sample audio\n",
        "# gender_prediction = predict_gender('Speaker_0000_00000.wav')\n",
        "# gender_prediction = predict_gender('Recording.m4a')\n",
        "# gender_prediction = predict_gender('Speaker0048_000.wav')\n",
        "# gender_prediction = predict_gender('Speaker0048_028.wav')\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/en_test_0/common_voice_en_18277778.mp3') # male\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/en_test_0/common_voice_en_22338655.mp3') # female\n",
        "\n",
        "print(f\"Predicted Gender: {gender_prediction}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spectrogram Implementation\n",
        "\n",
        "Didn't work because the feature extracted was too large and lower it resizing made it lose too much data."
      ],
      "metadata": {
        "id": "CooTR4JvAr0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "file_path = './drive/MyDrive/audio-project/common_test.tsv'\n",
        "# file_path = './drive/MyDrive/audio-project/common_train.tsv'\n",
        "df_raw = pd.read_csv(file_path, sep='\\t')\n",
        "print(len(df_raw))\n",
        "df_include_other = df_raw.dropna(subset=['gender'])\n",
        "print(len(df_include_other))\n",
        "df = df_include_other[df_include_other['gender'] != 'other']\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHW2U408-GLj",
        "outputId": "f7c015d3-3376-4a90-ac1c-5813e0fb42d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "16372\n",
            "2356\n",
            "2320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHWJq2QvxzmP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D\n",
        "from scipy.interpolate import interp2d\n",
        "\n",
        "# extract spectogram: Short-Time Fourier Transform\n",
        "# frequency bands / time segments\n",
        "def extract_features(file_path):\n",
        "    max_length = 250\n",
        "    # target_shape=(40, 40)\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_path)\n",
        "        # https://librosa.org/doc/main/generated/librosa.stft.html\n",
        "        # use stft. n_fft determines the column length: (n_fft/2)+1 = column length\n",
        "        # https://stackoverflow.com/questions/62584184/understanding-the-shape-of-spectrograms-and-n-mels\n",
        "        stft = np.abs(librosa.stft(audio, n_fft=512))\n",
        "\n",
        "        if stft.shape[1] < max_length:\n",
        "            pad_width = max_length - stft.shape[1]\n",
        "            stft = np.pad(stft, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            stft = stft[:, :max_length]\n",
        "\n",
        "        # print part of the spectrogram\n",
        "        print(\"spectogram:\")\n",
        "        print(f\"The sampling rate of the audio file is: {sample_rate} Hz\")\n",
        "        print(stft[:6, :6])\n",
        "        print(\"Shape of stft array:\", stft.shape)\n",
        "        print(\"Number of rows:\", stft.shape[0])\n",
        "        print(\"Number of columns:\", stft.shape[1])\n",
        "\n",
        "        return stft\n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file_path)\n",
        "        return None\n",
        "\n",
        "features = [] # contains audio features in 1D array\n",
        "labels_gender = [] # this will contain label for gender 1 to 1 of features\n",
        "\n",
        "# for _, row in df.iterrows():\n",
        "#     file_name = './drive/MyDrive/audio-project/en_test_0/' + row['path']\n",
        "#     mfccs = extract_features(file_name)\n",
        "#     if mfccs is not None:\n",
        "#         features.append(mfccs)\n",
        "#         if row['gender'].lower() == 'male':\n",
        "#             labels_gender.append(0)\n",
        "#         elif row['gender'].lower() == 'female':\n",
        "#             labels_gender.append(1)\n",
        "#         else:\n",
        "#             print(f\"Unexpected gender label: {row['gender']}\")\n",
        "counter = 0\n",
        "for _, row in df.iterrows():\n",
        "    if counter < 1000:  # only 5 audio\n",
        "        file_name = './drive/MyDrive/audio-project/en_test_0/' + row['path']\n",
        "        spectrogram = extract_features(file_name)\n",
        "        if spectrogram is not None:\n",
        "            features.append(spectrogram)\n",
        "            if row['gender'].lower() == 'male':\n",
        "                labels_gender.append(0)\n",
        "            elif row['gender'].lower() == 'female':\n",
        "                labels_gender.append(1)\n",
        "            else:\n",
        "                print(f\"Unexpected gender label: {row['gender']}\")\n",
        "        counter += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# make features, labels_gender to np array\n",
        "X = np.array(features) # 2D array\n",
        "y = np.array(labels_gender)\n",
        "\n",
        "# split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# reshape this to 3D to use Conv1D\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "print(\"X_train dimensions:\", X_train.shape)\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid')) # using binary because determining 0 - male or 1 - female\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=16, validation_split=0.2)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "# save the model\n",
        "model.save('gender_classification_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model = tf.keras.models.load_model('gender_classification_model.keras')\n",
        "\n",
        "def predict_gender(audio_file):\n",
        "\n",
        "    spectrogram = create_spectrogram(audio_file)\n",
        "\n",
        "    # spectrogram = resize_spectrogram_interpolation(spectrogram)\n",
        "\n",
        "    # spectrogram = max_pooling_spectrogram(spectrogram)\n",
        "\n",
        "    # reshape feature for the model\n",
        "    features_reshaped = np.expand_dims(spectrogram, axis=0)\n",
        "\n",
        "    # get the prediction\n",
        "    prediction = model.predict(features_reshaped)\n",
        "    print(prediction)\n",
        "\n",
        "    if prediction[0][0] >= 0.5:\n",
        "        return \"Female\"\n",
        "    else:\n",
        "        return \"Male\"\n",
        "\n",
        "# sample audio\n",
        "# gender_prediction = predict_gender('Speaker_0000_00000.wav')\n",
        "# gender_prediction = predict_gender('Speaker_0000_00001.wav')\n",
        "# gender_prediction = predict_gender('Speaker_0000_00002.wav')\n",
        "# gender_prediction = predict_gender('Speaker_0001_00000.wav')\n",
        "# gender_prediction = predict_gender('Recording.m4a')\n",
        "# gender_prediction = predict_gender('sally.m4a')\n",
        "# gender_prediction = predict_gender('Speaker0048_000.wav')\n",
        "# gender_prediction = predict_gender('Speaker0048_028.wav')\n",
        "# gender_prediction = predict_gender('Speaker0049_000.wav')\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/en_test_0/common_voice_en_18277778.mp3') # male\n",
        "# gender_prediction = predict_gender('./drive/MyDrive/audio-project/en_test_0/common_voice_en_22338655.mp3') # female\n",
        "\n",
        "\n",
        "print(f\"Predicted Gender: {gender_prediction}\")"
      ],
      "metadata": {
        "id": "Hs6DhtxGGrzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c755aef-7930-4ba5-fb77-3af415f21241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 176ms/step\n",
            "[[0.52720743]]\n",
            "Predicted Gender: Female\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_waveform_spectrogram_melspectrogram(file_path):\n",
        "    audio, sample_rate = librosa.load(file_path)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    librosa.display.waveshow(audio, sr=sample_rate)\n",
        "    plt.title('My Audio Waveform')\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.show()\n",
        "\n",
        "    stft = np.abs(librosa.stft(audio))\n",
        "    db_stft = librosa.amplitude_to_db(stft, ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    librosa.display.specshow(db_stft, sr=sample_rate, x_axis='time', y_axis='hz')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('My Spectrogram')\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "    plt.show()\n",
        "\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n",
        "    db_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    librosa.display.specshow(db_mel_spectrogram, sr=sample_rate, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('My Mel-Spectrogram')\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylabel('Mel')\n",
        "    plt.show()\n",
        "\n",
        "file_path = './drive/MyDrive/audio-project/en_test_0/common_voice_en_18277778.mp3'\n",
        "plot_waveform_spectrogram_melspectrogram(file_path)"
      ],
      "metadata": {
        "id": "nf7NjGSVx1Lo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}